{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad676a1-a636-47e2-8f6c-bceaeb39d12f",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "*The purpose of this file is to represent the research I've learnt on the inner workings of support vector machines. This will include what it is, how it works, how to optimize it, different functions, etc. The aim of this to gain a deep confident grasp of SVM's. This research will be related/connect to this titanic model I'm making.*\n",
    "\n",
    "# Table of Contents:\n",
    "- [Classification VS Regression](\"ClassVSReg\")\n",
    "- [What is a Support Vector Machine?](\"svm-intro\")\n",
    "- [SVM Basic Code Explanation](\"svm-intro-code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb3e52-57ae-4e5c-8b8a-e5623c42c342",
   "metadata": {},
   "source": [
    "<a name=\"ClassVSReg\"></a>\n",
    "# Classification VS Regression\n",
    "\n",
    "*Classification and regression are both types of supervised learning used in machine learning, where the goal is to train a model on labeled data to make predictions. The key difference lies in the type of output they produce: classification predicts discrete categories or class labels (such as 'spam' or 'not spam'), while regression predicts continuous numerical values (such as house prices or temperatures). Despite this difference, both approaches follow the same underlying process - learning a mapping from input features to an output - and use similar algorithms that are adapted to suit either categorical or continuous prediction tasks.*\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"classification.png\" alt=\"Classification\" width=\"300\"/>\n",
    "  <img src=\"regression.png\" alt=\"Regression\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "### Classification:\n",
    "Example:\n",
    "- A list of students in a class can be categorised by gender.\n",
    "- A dataset of images of hand-drawn numbers 0-9 can be classified into type integers.\n",
    "\n",
    "Common Usage:\n",
    "- Medical diagnostics\n",
    "- Identifying spam vs non-spam\n",
    "- Identifying whether a file is malicous\n",
    "- Image recognistion\n",
    "\n",
    "### Regression:\n",
    "Example:\n",
    "- Predicting the price of a house based on its features (size, location, number of rooms).\n",
    "- Estimating a person's weight based on height and age.\n",
    "\n",
    "Common Usage:\n",
    "- Forecasting stock prices or sales revenue\n",
    "- Predicting temperature or rainfall levels\n",
    "- Estimating delivery time or traffic flow\n",
    "- Modeling population growth over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82aee66-dcaa-42cc-abbf-edb6adf92849",
   "metadata": {},
   "source": [
    "<a name=\"svm-intro\"></a>\n",
    "# What is a Support Vector Machine?\n",
    "\n",
    "*A Support Vector Machine (SVM) is a supervised machine learning algorithm used for both classification and regression tasks. In this project, the focus will be on **classification**, not regression.*\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "SVM works by plotting data points in a multi-dimensional space, where each feature represents one axis (or dimension).  \n",
    "- For example: a dataset with two features like **height** and **weight** can be visualized in a 2D space.  \n",
    "- A dataset with three features would be 3D, and so on.\n",
    "\n",
    "The SVM algorithm then attempts to find the **best hyperplane** that separates the different classes of data. A hyperplane is a decision boundary:\n",
    "- In **2D**, it’s a line (1D hyperplane).\n",
    "- In **3D**, it’s a plane (2D hyperplane).\n",
    "- In general, the hyperplane is always **one dimension less** than the feature space.\n",
    "\n",
    "The key question is: **where should the hyperplane be placed?**\n",
    "\n",
    "SVM answers this by choosing the hyperplane that **maximizes the margin** — the distance between the hyperplane and the **nearest data points** from each class (called **support vectors**). This helps improve generalization to new data and makes SVM robust. The margin would be the distance between the hyperplane and the dotted line below.\n",
    "<p align=\"center\">\n",
    "  <img src=\"margin.png\" alt=\"margin\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "### Types of Margins\n",
    "*Hard Margin: No misclassifications allowed; assumes data is perfectly separable*\n",
    "\n",
    "*Soft Margin: Allows some misclassification; better for real-world noisy data*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340f83e-56de-467e-a65d-b917be8f1029",
   "metadata": {},
   "source": [
    "<a name=\"svm-intro-code\"></a>\n",
    "# SVM Basic Code Explanation:\n",
    "\n",
    "The first bit of every AI model consists of data loading and data exploration. This can easily be done with the use of python modules such as- Pandas, matplotlib, numpy and seaborn.\n",
    "\n",
    "Pandas is a fasts and power tool to help analyses data and is my prefered way to load data from a csv file. Pandas can be easily installed from the console through the Python Package Installer (pip):\n",
    "\n",
    "pip install pandas\n",
    "\n",
    "To import the pandas library and load the data we can use these lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884e408-92a0-4f90-b5df-e284740d2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv('titanic/train.csv') #Note that is pandas funciton to load csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd518dd-7872-4b33-8328-c1cae9e950a5",
   "metadata": {},
   "source": [
    "We can analyse this dataframe (df) with pandas functions such as head(), describe() info().\n",
    "\n",
    "**head()** shows the first 5 (defualt) rows of the dataframe, head(x) shows the first x rows of the dataframe. \n",
    "\n",
    "**describe()** shows each column's count(cnt), mean, standard deviation (std), minimum (min), 25th percentile (25%). 50th percentile (50%), 75th percentile (75%), and maximum (max).\n",
    "\n",
    "**info()** shows each column's index (#), Column label, number of non-null values, and data type (Dtype). It also states the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f94f4-898d-44c9-af30-d2abc3b5b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.describe()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff435a-d6cd-4028-b383-b496eed7c25f",
   "metadata": {},
   "source": [
    "The core idea behind AI models is prediction. We're training models on past (labeled) data with the goal of accurately predicting future or unseen data. This is essentially about learning which features (inputs) are most correlated with the target (output).\n",
    "\n",
    "At their heart, all machine learning models are just function estimators. In regression, we estimate a function that maps inputs to continuous outputs. In classification, we estimate a function that separates categories of data.\n",
    "\n",
    "That’s it - it’s all just functions.\n",
    "\n",
    "And that’s part of what makes AI so powerful: the world itself is governed by patterns and functions, and machines can often model these high-dimensional relationships far better than humans.\n",
    "\n",
    "The point of this (mini) rant is to highlight the importance of understanding which features are most correlated with the outcome - this is the foundation of good predictions and good models. The better we understand the data, the smarter our focus and efforts will be.\n",
    "\n",
    "Pandas has a cool function that maps correlation between all combinations of columns: ***corr()***.\n",
    "This function calculates the pairwise correlation between all numerical columns in the DataFrame, returning a matrix where each value ranges from -1 to 1:\n",
    "\n",
    "A value close to 1 means a strong positive correlation (as one feature increases, so does the other).\n",
    "\n",
    "A value close to -1 means a strong negative correlation (as one feature increases, the other decreases).\n",
    "\n",
    "A value close to 0 means little to no linear correlation.\n",
    "\n",
    "This is super useful when deciding which features are most influential or redundant, especially when trying to understand what drives predictions in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86235fa-2296-40cd-ae3c-52eaaeb1dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce61546-1967-48c4-8ace-50220de92432",
   "metadata": {},
   "source": [
    "However we need to make sure that all given column values are numeric, this can be done through a technique called encoding. One simple way to encode categorical values is with the map() function.\n",
    "It replaces each category with a number by applying a dictionary mapping to the column.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68ff95-a972-45af-8ee2-bcef6b75c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "# This replaces 'male' with 0 and 'female' with 1.\n",
    "\n",
    "corr_matrix = df[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare','Embarked_encoded', 'Sex_encoded']].corr()\n",
    "# Note that I encoded both 'embarked' and 'sex'. This returns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1bc1f-09ab-4c71-a325-267f993ce5c6",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"corr.png\" alt=\"margin\" width=\"450\"/>\n",
    "</p>\n",
    "\n",
    "This is good, we can now see the correlation between different columns, however we want to focus of features that correlate to surviving, so we need to filter out the rest. This can be done by **loc()**.\n",
    "\n",
    "loc() selects the row in the correlation matrix corresponding to 'Survived'.\n",
    "\n",
    "We can also filter out the correlation between Survived with itself, since its always 1. Using **drop()**\n",
    "\n",
    "drop() removes the correlation of Survived with itself (1.0), because it’s not useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b4855-cddd-4405-9a22-d26220075022",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_corr = corr_matrix.loc['Survived'].drop('Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0e611-10b2-4cbc-aaf8-4fb38db67843",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"corr_filtered.png\" alt=\"margin\" width=\"200\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49bc29-a5bc-4b49-936b-b2191b65c7d1",
   "metadata": {},
   "source": [
    "We can see now that the sex has the highest correlation to surviving, the second goes the Pclass. We want to look at the absolute values of the correlation matrix because it is important to consider the highest correltion whether that be positive or negative, as explained before, this indicates a stronger pattern.\n",
    "\n",
    "Now we can start programming the SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
